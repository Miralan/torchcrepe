import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class Crepe(nn.Module):
    """Crepe model definition"""
    
    def __init__(self, weight_file):
        super().__init__()
        # Load weights
        self.weights = np.load(weight_file, allow_pickle=True).item()

        # Model definition
        self.conv1 = self.conv(
            name='conv1',
            in_channels=1,
            out_channels=128,
            kernel_size=(512, 1),
            stride=(4, 1))
        self.conv1_BN = self.batch_normalization('conv1-BN', num_features=128)
        
        self.conv2 = self.conv(
            name='conv2',
            in_channels=128,
            out_channels=16,
            kernel_size=(64, 1))
        self.conv2_BN = self.batch_normalization('conv2-BN', num_features=16)
        
        self.conv3 = self.conv(
            name='conv3',
            in_channels=16,
            out_channels=16,
            kernel_size=(64, 1))
        self.conv3_BN = self.batch_normalization('conv3-BN', num_features=16)
        
        self.conv4 = self.conv(
            name='conv4',
            in_channels=16,
            out_channels=16,
            kernel_size=(64, 1))
        self.conv4_BN = self.batch_normalization('conv4-BN', num_features=16)
        
        self.conv5 = self.conv(
            name='conv5',
            in_channels=16,
            out_channels=32,
            kernel_size=(64, 1))
        self.conv5_BN = self.batch_normalization('conv5-BN', num_features=32)
        
        self.conv6 = self.conv(
            name='conv6',
            in_channels=32,
            out_channels=64,
            kernel_size=(64, 1))
        self.conv6_BN = self.batch_normalization('conv6-BN', num_features=64)
        
        self.classifier = self.dense(
            name='classifier',
            in_features=256,
            out_features=360)
        
    def forward(self, x, embed=False):
        # Forward pass through first four layers and part of layer five
        x = self.embed(x)
        
        if embed:
            return x
        
        # Finish layer five
        x = F.dropout(x, p=0.25, training=self.training, inplace=True)
        
        # Forward pass through layer six
        x = self.layer(x, self.conv6, self.conv6_BN)
        
        # Compute logits
        return self.classifier(x.reshape(-1, 256))

    ###########################################################################
    # Forward pass utilities
    ###########################################################################
        
    def embed(self, x):
        """Map input audio to pitch embedding"""
        # shape=(batch, 1, 1024, 1)
        x = x[:, None, :, None]
        
        # Forward pass through first four layers
        x = self.layer(x, self.conv1, self.conv1_BN, (0, 0, 254, 254))
        x = self.layer(x, self.conv2, self.conv2_BN)
        x = self.layer(x, self.conv3, self.conv3_BN)
        x = self.layer(x, self.conv4, self.conv4_BN)
        
        # Partial forward pass through layer five
        return self.layer_no_dropout(x, self.conv5, self.conv5_BN)
    
    def layer(self, x, conv, batch_norm, padding=(0, 0, 31, 32)):
        """Forward pass through one layer"""
        x = self.layer_no_dropout(x, conv, batch_norm, padding)
        return F.dropout(x, 0.25, self.training, inplace=True)
    
    def layer_no_dropout(self, x, conv, batch_norm, padding=(0, 0, 31, 32)):
        """Forward pass through one layer, without final dropout layer"""
        x = F.pad(x, padding)
        x = conv(x)
        x = F.relu(x)
        x = batch_norm(x)
        return F.max_pool2d(x, (2, 1), (2, 1))    
    
    ###########################################################################
    # Network construction (mostly autogenerated code by MMdnn)
    ###########################################################################
    
    def batch_normalization(self, name, **kwargs):
        """Create batch norm layer and load weights"""
        layer = nn.BatchNorm2d(
            **kwargs, eps=0.0010000000474974513, momentum=0.0)

        if 'scale' in self.weights[name]:
            layer.state_dict()['weight'].copy_(
                torch.from_numpy(self.weights[name]['scale']))
        else:
            layer.weight.data.fill_(1)

        if 'bias' in self.weights[name]:
            layer.state_dict()['bias'].copy_(
                torch.from_numpy(self.weights[name]['bias']))
        else:
            layer.bias.data.fill_(0)

        layer.state_dict()['running_mean'].copy_(
            torch.from_numpy(self.weights[name]['mean']))
        layer.state_dict()['running_var'].copy_(
            torch.from_numpy(self.weights[name]['var']))
        return layer

    def conv(self, name, **kwargs):
        """Create conv layer and load weights"""
        layer = nn.Conv2d(**kwargs)
        layer.state_dict()['weight'].copy_(
            torch.from_numpy(self.weights[name]['weights']))
        if 'bias' in self.weights[name]:
            layer.state_dict()['bias'].copy_(
                torch.from_numpy(self.weights[name]['bias']))
        return layer

    def dense(self, name, **kwargs):
        """Create dense layer and load weights"""
        layer = nn.Linear(**kwargs)
        layer.state_dict()['weight'].copy_(
            torch.from_numpy(self.weights[name]['weights']))
        if 'bias' in self.weights[name]:
            layer.state_dict()['bias'].copy_(
                torch.from_numpy(self.weights[name]['bias']))
        return layer
